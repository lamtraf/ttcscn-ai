{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lamtr\\anaconda3\\envs\\vasr\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "\n",
    "def convert_to_wav(source_path, target_path):\n",
    "    # Load the source file\n",
    "    audio = AudioSegment.from_file(source_path)\n",
    "    # Export as WAV\n",
    "    audio.export(target_path, format=\"wav\")\n",
    "    print(f\"Converted {source_path} to {target_path}\")\n",
    "\n",
    "\n",
    "def check_and_convert_to_wav(file_path):\n",
    "    # Check if the file is already a WAV file\n",
    "    if not file_path.lower().endswith('.wav'):\n",
    "        # Define a new file path with the WAV extension\n",
    "        base = os.path.splitext(file_path)[0]\n",
    "        new_file_path = base + \".wav\"\n",
    "        print(\"new file path: \", new_file_path)\n",
    "        # Convert to WAV\n",
    "        convert_to_wav(file_path, new_file_path)\n",
    "        return new_file_path\n",
    "    else:\n",
    "        print(f\"{file_path} is already in WAV format\")\n",
    "        return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lamtr\\anaconda3\\envs\\vasr\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\lamtr\\anaconda3\\envs\\vasr\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\lamtr\\anaconda3\\envs\\vasr\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Model: \"ASR\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, None, 80)]        0         \n",
      "                                                                 \n",
      " expand_dim (Reshape)        (None, None, 80, 1)       0         \n",
      "                                                                 \n",
      " conv_1 (Conv2D)             (None, None, 40, 32)      14432     \n",
      "                                                                 \n",
      " conv_1_bn (BatchNormalizat  (None, None, 40, 32)      128       \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " conv_1_relu (ReLU)          (None, None, 40, 32)      0         \n",
      "                                                                 \n",
      " conv_2 (Conv2D)             (None, None, 20, 32)      236544    \n",
      "                                                                 \n",
      " conv_2_bn (BatchNormalizat  (None, None, 20, 32)      128       \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " conv_2_relu (ReLU)          (None, None, 20, 32)      0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, None, 640)         0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, None, 1024)        3545088   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 1024)        0         \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, None, 1024)        4724736   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, None, 1024)        0         \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirecti  (None, None, 1024)        4724736   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, None, 1024)        0         \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirecti  (None, None, 1024)        4724736   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, None, 1024)        0         \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirecti  (None, None, 1024)        4724736   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, None, 1024)        1049600   \n",
      "                                                                 \n",
      " dense_1_relu (ReLU)         (None, None, 1024)        0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, None, 1024)        0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 128)         131200    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23876064 (91.08 MB)\n",
      "Trainable params: 23875936 (91.08 MB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "_________________________________________________________________\n",
      "The vocabulary is: ['', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'á', 'ã', 'ạ', 'ả', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'è', 'é', 'ẹ', 'ẻ', 'ẽ', 'ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'đ', 'ì', 'í', 'ĩ', 'ỉ', 'ị', 'ò', 'ó', 'õ', 'ọ', 'ỏ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'ù', 'ú', 'ũ', 'ụ', 'ủ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'ỳ', 'ỵ', 'ỷ', 'ỹ', 'ý', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', ' '] (size =127)\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import logging\n",
    "import string\n",
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import webrtcvad\n",
    "from tensorflow import keras\n",
    "\n",
    "import wav_split\n",
    "\n",
    "# Hide GPU from visible devices\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# An integer scalar Tensor. The window length in samples.\n",
    "frame_length = 256\n",
    "# An integer scalar Tensor. The number of samples to step.\n",
    "frame_step = 160\n",
    "# An integer scalar Tensor. The size of the FFT to apply.\n",
    "# If not provided, uses the smallest power of 2 enclosing frame_length.\n",
    "fft_length = 384\n",
    "\n",
    "lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80\n",
    "\n",
    "# Path to the H5 file\n",
    "model_path = \"C:\\LearnIT\\Speech_to_text\\mymodel (1).h5\"\n",
    "\n",
    "\n",
    "def CTCLoss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Define CTC loss function\n",
    "    \"\"\"\n",
    "    # Compute the training-time loss value\n",
    "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "    loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Recreate the exact same model, including its weights and the optimizer\n",
    "model = tf.keras.models.load_model(model_path, custom_objects={'CTCLoss': CTCLoss}, compile=False)\n",
    "# Show the model architecture\n",
    "model.summary()\n",
    "\n",
    "\n",
    "def encode_single_buffer(audio_numpy_array):\n",
    "\n",
    "    audio_tensor = tf.convert_to_tensor(audio_numpy_array, dtype=tf.string)\n",
    "    audio, sampling_rate = tf.audio.decode_wav(audio_tensor)\n",
    "\n",
    "    # Check for stereo audio (2 channels) and convert to mono if necessary\n",
    "    if audio.shape[-1] == 2:\n",
    "        # Convert stereo to mono by averaging both channels\n",
    "        audio = tf.reduce_mean(audio, axis=-1, keepdims=True)\n",
    "\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    # 3. Change type to float\n",
    "    audio = tf.cast(audio, tf.float32)\n",
    "    # 4. Get the spectrogram\n",
    "    stfts = tf.signal.stft(audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length)\n",
    "    # 5. We only need the magnitude, which can be derived by applying tf.abs\n",
    "    spectrogram = tf.abs(stfts)  # get absolute value of complex number\n",
    "    spectrogram = tf.math.pow(spectrogram, 2)  # get power\n",
    "\n",
    "    # 6. mel spectrogram\n",
    "    num_spectrogram_bins = stfts.shape[-1]\n",
    "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(num_mel_bins, num_spectrogram_bins,\n",
    "                                                                        sampling_rate, lower_edge_hertz,\n",
    "                                                                        upper_edge_hertz)\n",
    "    mel_spectrograms = tf.tensordot(spectrogram, linear_to_mel_weight_matrix, 1)\n",
    "    mel_spectrograms.set_shape(spectrogram.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:]))\n",
    "    # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\n",
    "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
    "\n",
    "    # 8. normalisation\n",
    "    means = tf.math.reduce_mean(log_mel_spectrograms, 1, keepdims=True)\n",
    "    stddevs = tf.math.reduce_std(log_mel_spectrograms, 1, keepdims=True)\n",
    "    log_mel_spectrograms = (log_mel_spectrograms - means) / (stddevs + 1e-10)\n",
    "\n",
    "    return log_mel_spectrograms\n",
    "\n",
    "\n",
    "def encode_single_file(wav_file):\n",
    "    \"\"\"\n",
    "    Describes the transformation that we apply to each element of our dataset\n",
    "    \"\"\"\n",
    "    ###########################################\n",
    "    ##  Process the Audio\n",
    "    ##########################################\n",
    "    # 1. Read wav file\n",
    "    file = tf.io.read_file(wav_file)\n",
    "    # 2. Decode the wav file\n",
    "    audio, sampling_rate = tf.audio.decode_wav(file)\n",
    "    # Check for stereo audio (2 channels) and convert to mono if necessary\n",
    "    if audio.shape[-1] == 2:\n",
    "        # Convert stereo to mono by averaging both channels\n",
    "        audio = tf.reduce_mean(audio, axis=-1, keepdims=True)\n",
    "\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    # 3. Change type to float\n",
    "    audio = tf.cast(audio, tf.float32)\n",
    "    # 4. Get the spectrogram\n",
    "    stfts = tf.signal.stft(audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length)\n",
    "    # 5. We only need the magnitude, which can be derived by applying tf.abs\n",
    "    spectrogram = tf.abs(stfts)  # get absolute value of complex number\n",
    "    spectrogram = tf.math.pow(spectrogram, 2)  # get power\n",
    "\n",
    "    # 6. mel spectrogram\n",
    "    num_spectrogram_bins = stfts.shape[-1]\n",
    "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(num_mel_bins, num_spectrogram_bins,\n",
    "                                                                        sampling_rate, lower_edge_hertz,\n",
    "                                                                        upper_edge_hertz)\n",
    "    mel_spectrograms = tf.tensordot(spectrogram, linear_to_mel_weight_matrix, 1)\n",
    "    mel_spectrograms.set_shape(spectrogram.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:]))\n",
    "    # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\n",
    "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
    "\n",
    "    # 8. normalisation\n",
    "    means = tf.math.reduce_mean(log_mel_spectrograms, 1, keepdims=True)\n",
    "    stddevs = tf.math.reduce_std(log_mel_spectrograms, 1, keepdims=True)\n",
    "    log_mel_spectrograms = (log_mel_spectrograms - means) / (stddevs + 1e-10)\n",
    "\n",
    "    return log_mel_spectrograms\n",
    "\n",
    "\n",
    "def pad_tensor(tensor, desired_size):\n",
    "    # Calculate the padding size\n",
    "    padding_size = max(desired_size - tf.shape(tensor)[-1], 0)\n",
    "    # Pad the tensor to the desired size\n",
    "    padded_tensor = tf.pad(tensor, [[0, 0], [0, padding_size]])\n",
    "    return padded_tensor\n",
    "\n",
    "\n",
    "def convert_to_tensor_from_frame(audio):\n",
    "    encoded = encode_single_buffer(audio)\n",
    "    padded_features = pad_tensor(encoded, 16)\n",
    "    padded_features = np.expand_dims(padded_features, axis=0)\n",
    "    tensor_input = tf.convert_to_tensor(padded_features, dtype=tf.float32)\n",
    "    return tensor_input\n",
    "\n",
    "\n",
    "def convert_to_tensor_from_file(audio_file):\n",
    "    encoded = encode_single_file(audio_file)\n",
    "    padded_features = pad_tensor(encoded, 16)\n",
    "    padded_features = np.expand_dims(padded_features, axis=0)\n",
    "    tensor_input = tf.convert_to_tensor(padded_features, dtype=tf.float32)\n",
    "    return tensor_input\n",
    "\n",
    "\n",
    "# A utility function to decode the output of the network\n",
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    # Use greedy search. For complex tasks, you can use beam search\n",
    "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n",
    "    # Iterate over the results and get back the text\n",
    "    output_text = []\n",
    "    for result in results:\n",
    "        result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n",
    "        output_text.append(result)\n",
    "    return output_text\n",
    "\n",
    "\n",
    "def stt_from_frames(audio):\n",
    "    input_tensor = convert_to_tensor_from_frame(audio)\n",
    "    inference_time = 0.0\n",
    "\n",
    "    # Run Deepspeech\n",
    "    logging.debug('Running inference...')\n",
    "    inference_start = timer()\n",
    "\n",
    "    output = model.predict(input_tensor)\n",
    "    output = decode_batch_predictions(output)\n",
    "\n",
    "    inference_end = timer() - inference_start\n",
    "    inference_time += inference_end\n",
    "    logging.debug('Inference took %0.3fs.' % inference_end)\n",
    "\n",
    "    return [output, inference_time]\n",
    "\n",
    "\n",
    "def stt_from_file(audio):\n",
    "    input_tensor = convert_to_tensor_from_file(audio)\n",
    "    inference_time = 0.0\n",
    "\n",
    "    # Run Deepspeech\n",
    "    logging.debug('Running inference...')\n",
    "    inference_start = timer()\n",
    "\n",
    "    output = model.predict(input_tensor)\n",
    "    output = decode_batch_predictions(output)\n",
    "\n",
    "    inference_end = timer() - inference_start\n",
    "    inference_time += inference_end\n",
    "    logging.debug('Inference took %0.3fs.' % inference_end)\n",
    "\n",
    "    return [output, inference_time]\n",
    "\n",
    "\n",
    "lowercase_chars = string.ascii_lowercase\n",
    "accented_chars = \"àáãạảăắằẳẵặâấầẩẫậèéẹẻẽêềếểễệđìíĩỉịòóõọỏôốồổỗộơớờởỡợùúũụủưứừửữựỳỵỷỹý\"\n",
    "punctuation_chars = string.punctuation\n",
    "final_chars = lowercase_chars + accented_chars + punctuation_chars + \" \"\n",
    "characters = [x for x in final_chars]\n",
    "# Mapping characters to integers\n",
    "char_to_num = keras.layers.StringLookup(vocabulary=characters, oov_token=\"\")\n",
    "list = char_to_num.get_vocabulary()\n",
    "char2num =  \" \".join(list)\n",
    "char_to_num_ = char2num.encode(\"utf-8\")\n",
    "# Mapping integers back to original characters\n",
    "num_to_char = keras.layers.StringLookup(vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True)\n",
    "\n",
    "print(f\"The vocabulary is: {char_to_num.get_vocabulary()} \"\n",
    "      f\"(size ={char_to_num.vocabulary_size()})\")\n",
    "\n",
    "\n",
    "def vad_segment_generator(wavFile, aggressiveness):\n",
    "    logging.debug(\"Caught the wav file @: %s\" % (wavFile))\n",
    "    audio, sample_rate, audio_length = wav_split.read_wave(wavFile)\n",
    "    assert sample_rate == 16000, \"Only 16000Hz input WAV files are supported for now!\"\n",
    "    vad = webrtcvad.Vad(int(aggressiveness))\n",
    "    frames = wav_split.frame_generator(30, audio, sample_rate)\n",
    "    frames = list(frames)\n",
    "    segments = wav_split.vad_collector(sample_rate, 30, 300, vad, frames)\n",
    "\n",
    "    return segments, sample_rate, audio_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence, detect_nonsilent\n",
    "\n",
    "\n",
    "def split_audio_on_silence(audio_path, target_sample_rate=16000, silence_thresh=-50, min_silence_len=1000,\n",
    "                           keep_silence=500):\n",
    "    \"\"\"\n",
    "    Splits an audio file into segments based on silence.\n",
    "\n",
    "    :param audio_path: Path to the audio file.\n",
    "    :param silence_thresh: Silence threshold in dB. Lower values mean more silence will be detected.\n",
    "    :param min_silence_len: Minimum length of silence in milliseconds to consider as a split.\n",
    "    :param keep_silence: Amount of silence to leave at the beginning and end of each segment.\n",
    "    :return: List of audio segments.\n",
    "    \"\"\"\n",
    "    sound = AudioSegment.from_file(audio_path)\n",
    "    sound = sound.set_frame_rate(target_sample_rate)\n",
    "\n",
    "    # Split on silence\n",
    "    segments = split_on_silence(sound, min_silence_len=min_silence_len, silence_thresh=silence_thresh,\n",
    "                                keep_silence=keep_silence)\n",
    "    return segments\n",
    "\n",
    "\n",
    "def segment_audio_on_silence(file_path, min_silence_len=1000, silence_thresh=-40, max_segment_duration=3000):\n",
    "    \"\"\"\n",
    "    Segment an audio file based on silence and maximum segment duration.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): Path to the audio file.\n",
    "    min_silence_len (int): Minimum length of silence to be used for splitting (in ms).\n",
    "    silence_thresh (int): Silence threshold (in dB).\n",
    "    max_segment_duration (int): Maximum duration of a segment (in ms).\n",
    "\n",
    "    Returns:\n",
    "    List of np.ndarray: Each element is a numpy array representing a segment.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    audio = AudioSegment.from_file(file_path)\n",
    "\n",
    "    # Detect non-silent chunks\n",
    "    nonsilent_chunks = detect_nonsilent(audio, min_silence_len=min_silence_len, silence_thresh=silence_thresh)\n",
    "\n",
    "    # Initialize list to hold final segments\n",
    "    final_segments = []\n",
    "\n",
    "    for start_i, end_i in nonsilent_chunks:\n",
    "        segment = audio[start_i:end_i]\n",
    "        # Further split segment if it's longer than the max duration\n",
    "        while len(segment) > max_segment_duration:\n",
    "            # Split at the max duration point\n",
    "            split_segment, segment = segment[:max_segment_duration], segment[max_segment_duration:]\n",
    "            final_segments.append(np.array(split_segment.get_array_of_samples()))\n",
    "        final_segments.append(np.array(segment.get_array_of_samples()))\n",
    "\n",
    "    return final_segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import contextlib\n",
    "import wave\n",
    "import numpy as np\n",
    "from scipy.signal import resample_poly\n",
    "\n",
    "# def read_wave(path):\n",
    "#     \"\"\"Reads a .wav file.\n",
    "#\n",
    "#     Takes the path, and returns (PCM audio data, sample rate).\n",
    "#     \"\"\"\n",
    "#     with contextlib.closing(wave.open(path, 'rb')) as wf:\n",
    "#         num_channels = wf.getnchannels()\n",
    "#         assert num_channels == 1\n",
    "#         sample_width = wf.getsampwidth()\n",
    "#         assert sample_width == 2\n",
    "#         sample_rate = wf.getframerate()\n",
    "#         assert sample_rate in (8000, 16000, 32000)\n",
    "#         frames = wf.getnframes()\n",
    "#         pcm_data = wf.readframes(frames)\n",
    "#         duration = frames / sample_rate\n",
    "#         return pcm_data, sample_rate, duration\n",
    "\n",
    "def read_wave(path, target_sample_rate=16000):\n",
    "    \"\"\"Reads a .wav file and resamples it to the target sample rate.\n",
    "\n",
    "    Takes the path and target sample rate, and returns (PCM audio data, target sample rate, duration).\n",
    "    \"\"\"\n",
    "    with contextlib.closing(wave.open(path, 'rb')) as wf:\n",
    "        num_channels, sample_width, sample_rate, frames, _, _ = wf.getparams()\n",
    "        assert sample_width == 2, \"Only supports 16-bit audio.\"\n",
    "\n",
    "        # Read frames and convert to byte array\n",
    "        pcm_data = wf.readframes(frames)\n",
    "\n",
    "        # Convert byte array to numpy array\n",
    "        pcm_array = np.frombuffer(pcm_data, dtype=np.int16)\n",
    "\n",
    "        # Check if the audio is mono or stereo and convert to mono if necessary\n",
    "        if num_channels == 2:\n",
    "            pcm_array = pcm_array.reshape(-1, 2).mean(axis=1).astype(np.int16)\n",
    "\n",
    "        # Resample if the sample rate is not one of the expected rates\n",
    "        if sample_rate not in (8000, 16000, 32000):\n",
    "            # Calculate the number of output samples\n",
    "            num_output_samples = int(frames * target_sample_rate / sample_rate)\n",
    "            pcm_array = resample_poly(pcm_array, target_sample_rate, sample_rate)\n",
    "            pcm_array = np.round(pcm_array).astype(np.int16)  # Ensure it's int16\n",
    "\n",
    "        duration = len(pcm_array) / target_sample_rate\n",
    "        return pcm_array.tobytes(), target_sample_rate, duration\n",
    "\n",
    "\n",
    "def write_wave(path, audio, sample_rate):\n",
    "    \"\"\"Writes a .wav file.\n",
    "\n",
    "    Takes path, PCM audio data, and sample rate.\n",
    "    \"\"\"\n",
    "    with contextlib.closing(wave.open(path, 'wb')) as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(2)\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(audio)\n",
    "\n",
    "\n",
    "class Frame(object):\n",
    "    \"\"\"Represents a \"frame\" of audio data.\"\"\"\n",
    "\n",
    "    def __init__(self, bytes, timestamp, duration):\n",
    "        self.bytes = bytes\n",
    "        self.timestamp = timestamp\n",
    "        self.duration = duration\n",
    "\n",
    "\n",
    "def frame_generator(frame_duration_ms, audio, sample_rate):\n",
    "    \"\"\"Generates audio frames from PCM audio data.\n",
    "\n",
    "    Takes the desired frame duration in milliseconds, the PCM data, and\n",
    "    the sample rate.\n",
    "\n",
    "    Yields Frames of the requested duration.\n",
    "    \"\"\"\n",
    "    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\n",
    "    offset = 0\n",
    "    timestamp = 0.0\n",
    "    duration = (float(n) / sample_rate) / 2.0\n",
    "    while offset + n < len(audio):\n",
    "        yield Frame(audio[offset:offset + n], timestamp, duration)\n",
    "        timestamp += duration\n",
    "        offset += n\n",
    "\n",
    "\n",
    "def vad_collector(sample_rate, frame_duration_ms, padding_duration_ms, vad, frames):\n",
    "    \"\"\"Filters out non-voiced audio frames.\n",
    "\n",
    "    Given a webrtcvad.Vad and a source of audio frames, yields only\n",
    "    the voiced audio.\n",
    "\n",
    "    Uses a padded, sliding window algorithm over the audio frames.\n",
    "    When more than 90% of the frames in the window are voiced (as\n",
    "    reported by the VAD), the collector triggers and begins yielding\n",
    "    audio frames. Then the collector waits until 90% of the frames in\n",
    "    the window are unvoiced to detrigger.\n",
    "\n",
    "    The window is padded at the front and back to provide a small\n",
    "    amount of silence or the beginnings/endings of speech around the\n",
    "    voiced frames.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    sample_rate - The audio sample rate, in Hz.\n",
    "    frame_duration_ms - The frame duration in milliseconds.\n",
    "    padding_duration_ms - The amount to pad the window, in milliseconds.\n",
    "    vad - An instance of webrtcvad.Vad.\n",
    "    frames - a source of audio frames (sequence or generator).\n",
    "\n",
    "    Returns: A generator that yields PCM audio data.\n",
    "    \"\"\"\n",
    "    num_padding_frames = int(padding_duration_ms / frame_duration_ms)\n",
    "    # We use a deque for our sliding window/ring buffer.\n",
    "    ring_buffer = collections.deque(maxlen=num_padding_frames)\n",
    "    # We have two states: TRIGGERED and NOTTRIGGERED. We start in the\n",
    "    # NOTTRIGGERED state.\n",
    "    triggered = False\n",
    "\n",
    "    voiced_frames = []\n",
    "    for frame in frames:\n",
    "        is_speech = vad.is_speech(frame.bytes, sample_rate)\n",
    "\n",
    "        if not triggered:\n",
    "            ring_buffer.append((frame, is_speech))\n",
    "            num_voiced = len([f for f, speech in ring_buffer if speech])\n",
    "            # If we're NOTTRIGGERED and more than 90% of the frames in\n",
    "            # the ring buffer are voiced frames, then enter the\n",
    "            # TRIGGERED state.\n",
    "            if num_voiced > 0.9 * ring_buffer.maxlen:\n",
    "                triggered = True\n",
    "                # We want to yield all the audio we see from now until\n",
    "                # we are NOTTRIGGERED, but we have to start with the\n",
    "                # audio that's already in the ring buffer.\n",
    "                for f, s in ring_buffer:\n",
    "                    voiced_frames.append(f)\n",
    "                ring_buffer.clear()\n",
    "        else:\n",
    "            # We're in the TRIGGERED state, so collect the audio data\n",
    "            # and add it to the ring buffer.\n",
    "            voiced_frames.append(frame)\n",
    "            ring_buffer.append((frame, is_speech))\n",
    "            num_unvoiced = len([f for f, speech in ring_buffer if not speech])\n",
    "            # If more than 90% of the frames in the ring buffer are\n",
    "            # unvoiced, then enter NOTTRIGGERED and yield whatever\n",
    "            # audio we've collected.\n",
    "            if num_unvoiced > 0.9 * ring_buffer.maxlen:\n",
    "                triggered = False\n",
    "                yield b''.join([f.bytes for f in voiced_frames])\n",
    "                ring_buffer.clear()\n",
    "                voiced_frames = []\n",
    "    if triggered:\n",
    "        pass\n",
    "    # If we have any leftover voiced audio when we run out of input,\n",
    "    # yield it.\n",
    "    if voiced_frames:\n",
    "        yield b''.join([f.bytes for f in voiced_frames])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIVOSDEV14_116.wav\n",
      "./audio_sample/VIVOSDEV14_116.wav is already in WAV format\n",
      "1/1 [==============================] - 0s 472ms/step\n",
      "output:mụt gấp cầu cá trình tàu sài gòn ảnh may qình\n",
      "inference_time: 0.5721323000002485\n",
      "Prediction Ended!\n"
     ]
    }
   ],
   "source": [
    "# Import the argparse library\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import textwrap\n",
    "import wave\n",
    "\n",
    "import audio_converter\n",
    "import model\n",
    "import split\n",
    "\n",
    "sample_rate = 16000\n",
    "buffer_size = 2048 * 10\n",
    "num_channels = 1\n",
    "sample_width = 2  # Sample width in bytes\n",
    "\n",
    "\n",
    "def predict_small_file(file_name):\n",
    "    path = './audio_sample/' + file_name\n",
    "\n",
    "    print(file_name)\n",
    "    output_file_path = file_name.rstrip(\".wav\") + \".txt\"\n",
    "    output_file_path = 'output/' + output_file_path\n",
    "\n",
    "    convertedFile = audio_converter.check_and_convert_to_wav(path)\n",
    "    output, inference_time = model.stt_from_file(convertedFile)\n",
    "\n",
    "    print(\"output:\" + output[0] + \"\\ninference_time: \" + str(inference_time))\n",
    "\n",
    "    combined_transcription = textwrap.fill(output[0], width=70)\n",
    "\n",
    "    combined_transcription += '\\nCompleted in ' + str(inference_time)\n",
    "    with open(output_file_path, 'w+', encoding=\"utf-8\") as file:\n",
    "        file.write(combined_transcription)\n",
    "\n",
    "    print(\"Prediction Ended!\")\n",
    "\n",
    "\n",
    "def predict_big_file(path):\n",
    "    path = './audio_sample/' + path\n",
    "\n",
    "    output_file_path = path.rstrip(\".wav\") + \".txt\"\n",
    "    output_file_path = 'output/' + output_file_path\n",
    "\n",
    "    convertedFile = audio_converter.check_and_convert_to_wav(path)\n",
    "\n",
    "    # segments = split.split_audio_on_silence(convertedFile)\n",
    "    segments = split.segment_audio_on_silence(convertedFile)\n",
    "    transcriptions = []\n",
    "    predict_times = 0\n",
    "    chunks = 0\n",
    "    for i, segment in enumerate(segments):\n",
    "        print(\"Processing chunk \", i)\n",
    "        chunks += 1\n",
    "        # Run deepspeech on the chunk that just completed VAD\n",
    "        logging.debug(\"Processing chunk %002d\" % (i,))\n",
    "\n",
    "        # Convert the NumPy array to bytes\n",
    "        audio_bytes = segment.tobytes()\n",
    "\n",
    "        # Create a wave file and set the parameters\n",
    "        with wave.open('temp1.wav', 'wb') as audio_file:\n",
    "            audio_file.setnchannels(num_channels)\n",
    "            audio_file.setsampwidth(sample_width)\n",
    "            audio_file.setframerate(sample_rate)\n",
    "\n",
    "            # Write audio data\n",
    "            audio_file.writeframes(audio_bytes)\n",
    "\n",
    "        # Perform speech-to-text\n",
    "        output, inference_time = model.stt_from_file('temp.wav')\n",
    "\n",
    "        print(\"output:\" + output[0] + \"\\ninference_time: \" + str(inference_time))\n",
    "        transcriptions.extend(output[0])\n",
    "        predict_times += inference_time\n",
    "\n",
    "    combined_transcription = ''.join(transcriptions)\n",
    "    combined_transcription = textwrap.fill(combined_transcription, width=70)\n",
    "\n",
    "    combined_transcription += '\\nCompleted in ' + str(predict_times) + \"\\nChunk: \" + str(chunks)\n",
    "    with open(output_file_path, 'w', encoding=\"utf-8\") as file:\n",
    "        file.write(combined_transcription)\n",
    "\n",
    "    print(\"Prediction Ended!\")\n",
    "\n",
    "def main_predict_demo(file_path, smalll=False):\n",
    "    if (smalll):\n",
    "        predict_small_file(file_path)\n",
    "    else:\n",
    "        predict_big_file(file_path)\n",
    "\n",
    "\n",
    "def main():\n",
    "    path = \"VIVOSDEV14_116.wav\"\n",
    "    main_predict_demo(path, smalll=True)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vasr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
